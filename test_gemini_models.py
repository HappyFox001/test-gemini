#!/usr/bin/env python3
"""
Gemini æ¨¡å‹å»¶æ—¶æ€§èƒ½æµ‹è¯•
æµ‹è¯•é¦–å­—å»¶æ—¶ã€æ€»å“åº”æ—¶é—´ã€å¯¹è¯é—´éš”
"""

import time
import json
import os
from datetime import datetime
from google import genai
from typing import Dict, Tuple

# åˆå§‹åŒ–å®¢æˆ·ç«¯
API_KEY = os.getenv('GEMINI_API_KEY')
if not API_KEY:
    raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ GEMINI_API_KEY")

client = genai.Client(api_key=API_KEY)

# æµ‹è¯•æ¨¡å‹åˆ—è¡¨
MODELS = [
    "gemini-3-flash-preview",
    "gemini-3-pro-preview",
    "gemini-2.5-pro",
    "gemini-2.5-flash"
]

# ä¸¤è½®å¯¹è¯çš„æç¤ºè¯
PROMPTS = [
    "ä½ å¥½ï¼Œæˆ‘æœ€è¿‘å·¥ä½œå‹åŠ›å¾ˆå¤§ï¼Œæ„Ÿè§‰å¾ˆç„¦è™‘ã€‚",
    "è°¢è°¢ä½ çš„å»ºè®®ï¼Œæˆ‘è¯¥å¦‚ä½•å¼€å§‹æ”¹å–„è¿™ä¸ªçŠ¶å†µå‘¢ï¼Ÿ"
]

# å¯¹è¯é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
CONVERSATION_GAP = 0.5


def test_model_with_timing(model: str, prompt: str) -> Dict:
    """
    æµ‹è¯•å•ä¸ªæ¨¡å‹çš„å“åº”ï¼Œè®°å½•è¯¦ç»†æ—¶é—´æ•°æ®

    Returns:
        {
            'first_token_time': é¦–å­—å»¶æ—¶ï¼ˆç§’ï¼‰,
            'total_time': æ€»å“åº”æ—¶é—´ï¼ˆç§’ï¼‰,
            'response_length': å“åº”å­—ç¬¦æ•°,
            'response_text': å“åº”å†…å®¹
        }
    """
    start_time = time.time()
    first_token_time = None
    response_text = ""

    try:
        # ä½¿ç”¨æµå¼å“åº”æ¥è·å–é¦–å­—å»¶æ—¶
        response = client.models.generate_content_stream(
            model=model,
            contents=prompt,
        )

        # æ¥æ”¶æµå¼å“åº”
        for chunk in response:
            # è®°å½•é¦–å­—å»¶æ—¶ï¼ˆç¬¬ä¸€ä¸ª chunk åˆ°è¾¾æ—¶é—´ï¼‰
            if first_token_time is None:
                first_token_time = time.time() - start_time

            # æå–æ–‡æœ¬å†…å®¹
            if hasattr(chunk, 'candidates') and chunk.candidates:
                candidate = chunk.candidates[0]
                if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                    for part in candidate.content.parts:
                        # åªæå–æ–‡æœ¬éƒ¨åˆ†ï¼Œå¿½ç•¥ thought_signature ç­‰
                        if hasattr(part, 'text') and part.text:
                            response_text += part.text

        total_time = time.time() - start_time

        return {
            'first_token_time': first_token_time or 0,
            'total_time': total_time,
            'response_length': len(response_text),
            'response_text': response_text
        }

    except Exception as e:
        total_time = time.time() - start_time
        return {
            'first_token_time': 0,
            'total_time': total_time,
            'response_length': 0,
            'response_text': f"é”™è¯¯: {str(e)}",
            'error': str(e)
        }


def run_performance_test():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("\n" + "=" * 80)
    print("ğŸš€ Gemini æ¨¡å‹å»¶æ—¶æ€§èƒ½æµ‹è¯•")
    print("=" * 80)
    print(f"ğŸ“… æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ æµ‹è¯•æ¨¡å‹: {', '.join(MODELS)}")
    print(f"ğŸ’¬ å¯¹è¯è½®æ•°: {len(PROMPTS)}")
    print(f"â±ï¸  å¯¹è¯é—´éš”: {CONVERSATION_GAP}ç§’")
    print("=" * 80 + "\n")

    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = {}

    # æµ‹è¯•æ¯ä¸ªæ¨¡å‹
    for model in MODELS:
        print(f"ğŸ“Š æµ‹è¯•æ¨¡å‹: {model}")
        print("-" * 80)

        model_results = {
            'conversations': [],
            'total_length': 0,
            'total_time': 0,
            'gaps': []
        }

        # è¿›è¡Œä¸¤è½®å¯¹è¯
        for round_num, prompt in enumerate(PROMPTS, 1):
            print(f"\nç¬¬ {round_num} è½®å¯¹è¯...")
            print(f"æç¤ºè¯: {prompt}")

            # æµ‹è¯•å“åº”
            result = test_model_with_timing(model, prompt)

            # æ‰“å°ç»“æœ
            print(f"â”œâ”€ é¦–å­—å»¶æ—¶: {result['first_token_time']:.3f}ç§’")
            print(f"â”œâ”€ æ€»å“åº”æ—¶é—´: {result['total_time']:.3f}ç§’")
            print(f"â””â”€ å“åº”é•¿åº¦: {result['response_length']}å­—ç¬¦")

            model_results['conversations'].append(result)
            model_results['total_length'] += result['response_length']
            model_results['total_time'] += result['total_time']

            # å¦‚æœä¸æ˜¯æœ€åä¸€è½®ï¼Œç­‰å¾…å¹¶è®°å½•é—´éš”
            if round_num < len(PROMPTS):
                gap_start = time.time()
                time.sleep(CONVERSATION_GAP)
                actual_gap = time.time() - gap_start
                model_results['gaps'].append(actual_gap)
                print(f"\nâ¸ï¸  å¯¹è¯é—´éš”: {actual_gap:.3f}ç§’")

        all_results[model] = model_results
        print("\n" + "=" * 80 + "\n")

    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print_comparison_table(all_results)

    # ä¿å­˜ç»“æœ
    save_results(all_results)

    return all_results


def print_comparison_table(results: Dict):
    """æ‰“å°æ€§èƒ½å¯¹æ¯”è¡¨æ ¼"""
    print("\n" + "=" * 80)
    print("ğŸ“Š æ€§èƒ½å¯¹æ¯”æ€»ç»“")
    print("=" * 80 + "\n")

    # è¡¨å¤´
    print(f"{'æŒ‡æ ‡':<20} {'gemini-3-flash':<20} {'gemini-3-pro':<20}")
    print("-" * 80)

    # æå–ä¸¤ä¸ªæ¨¡å‹çš„ç»“æœ
    flash_key = "gemini-3-flash-preview"
    pro_key = "gemini-3-pro-preview"

    flash_results = results.get(flash_key, {})
    pro_results = results.get(pro_key, {})

    # ç¬¬ä¸€è½®å¯¹è¯æ•°æ®
    flash_conv1 = flash_results.get('conversations', [{}])[0]
    pro_conv1 = pro_results.get('conversations', [{}])[0]

    print(f"{'[ç¬¬1è½®] é¦–å­—å»¶æ—¶':<20} {flash_conv1.get('first_token_time', 0):.3f}ç§’{'':<13} {pro_conv1.get('first_token_time', 0):.3f}ç§’")
    print(f"{'[ç¬¬1è½®] æ€»æ—¶é—´':<20} {flash_conv1.get('total_time', 0):.3f}ç§’{'':<13} {pro_conv1.get('total_time', 0):.3f}ç§’")
    print(f"{'[ç¬¬1è½®] å“åº”é•¿åº¦':<20} {flash_conv1.get('response_length', 0)}å­—ç¬¦{'':<13} {pro_conv1.get('response_length', 0)}å­—ç¬¦")

    # å¯¹è¯é—´éš”
    flash_gap = flash_results.get('gaps', [0])[0] if flash_results.get('gaps') else 0
    pro_gap = pro_results.get('gaps', [0])[0] if pro_results.get('gaps') else 0
    print(f"{'å¯¹è¯é—´éš”':<20} {flash_gap:.3f}ç§’{'':<13} {pro_gap:.3f}ç§’")

    # ç¬¬äºŒè½®å¯¹è¯æ•°æ®
    flash_conv2 = flash_results.get('conversations', [{}])[1] if len(flash_results.get('conversations', [])) > 1 else {}
    pro_conv2 = pro_results.get('conversations', [{}])[1] if len(pro_results.get('conversations', [])) > 1 else {}

    print(f"{'[ç¬¬2è½®] é¦–å­—å»¶æ—¶':<20} {flash_conv2.get('first_token_time', 0):.3f}ç§’{'':<13} {pro_conv2.get('first_token_time', 0):.3f}ç§’")
    print(f"{'[ç¬¬2è½®] æ€»æ—¶é—´':<20} {flash_conv2.get('total_time', 0):.3f}ç§’{'':<13} {pro_conv2.get('total_time', 0):.3f}ç§’")
    print(f"{'[ç¬¬2è½®] å“åº”é•¿åº¦':<20} {flash_conv2.get('response_length', 0)}å­—ç¬¦{'':<13} {pro_conv2.get('response_length', 0)}å­—ç¬¦")

    print("-" * 80)

    # æ€»è®¡
    print(f"{'æ€»å“åº”é•¿åº¦':<20} {flash_results.get('total_length', 0)}å­—ç¬¦{'':<13} {pro_results.get('total_length', 0)}å­—ç¬¦")
    print(f"{'æ€»å“åº”æ—¶é—´':<20} {flash_results.get('total_time', 0):.3f}ç§’{'':<13} {pro_results.get('total_time', 0):.3f}ç§’")

    # è®¡ç®—å¹³å‡é€Ÿåº¦ï¼ˆå­—ç¬¦/ç§’ï¼‰
    flash_speed = flash_results.get('total_length', 0) / flash_results.get('total_time', 1) if flash_results.get('total_time', 0) > 0 else 0
    pro_speed = pro_results.get('total_length', 0) / pro_results.get('total_time', 1) if pro_results.get('total_time', 0) > 0 else 0

    print(f"{'å¹³å‡é€Ÿåº¦':<20} {flash_speed:.1f}å­—ç¬¦/ç§’{'':<8} {pro_speed:.1f}å­—ç¬¦/ç§’")

    print("\n" + "=" * 80 + "\n")


def save_results(results: Dict):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"performance_test_{timestamp}.json"

    # ç®€åŒ–è¾“å‡ºï¼Œåªä¿ç•™æ—¶é—´æ•°æ®
    simplified_results = {}

    for model, data in results.items():
        simplified_results[model] = {
            'total_length': data['total_length'],
            'total_time': data['total_time'],
            'conversations': [
                {
                    'first_token_time': conv.get('first_token_time', 0),
                    'total_time': conv.get('total_time', 0),
                    'response_length': conv.get('response_length', 0)
                }
                for conv in data['conversations']
            ],
            'gaps': data.get('gaps', [])
        }

    output = {
        'timestamp': timestamp,
        'test_type': 'latency_performance',
        'models': MODELS,
        'conversation_rounds': len(PROMPTS),
        'results': simplified_results
    }

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"âœ… æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}\n")


if __name__ == "__main__":
    try:
        run_performance_test()
        print("âœ¨ æµ‹è¯•å®Œæˆï¼\n")
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•å·²è¢«ç”¨æˆ·ä¸­æ–­\n")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•å‡ºé”™: {str(e)}\n")
        import traceback
        traceback.print_exc()
