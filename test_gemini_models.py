#!/usr/bin/env python3
"""
Gemini æ¨¡å‹å»¶æ—¶æ€§èƒ½æµ‹è¯•
æµ‹è¯•é¦–å­—å»¶æ—¶ã€æ€»å“åº”æ—¶é—´ã€å¯¹è¯é—´éš”
"""

import time
import json
import os
from datetime import datetime
from google import genai
from google.genai import types
from typing import Dict, Tuple, Optional

# åˆå§‹åŒ–å®¢æˆ·ç«¯
API_KEY = os.getenv('GEMINI_API_KEY')
if not API_KEY:
    raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ GEMINI_API_KEY")

client = genai.Client(api_key=API_KEY)

# æµ‹è¯•æ¨¡å‹é…ç½®åˆ—è¡¨ï¼ˆä¸²è¡Œæµ‹è¯•ï¼Œä¸€ä¸ªå®Œæˆåæ‰æµ‹ä¸‹ä¸€ä¸ªï¼‰
# æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼šhttps://ai.google.dev/gemini-api/docs/thinking
# - Gemini 3 ä½¿ç”¨ thinking_level: "minimal", "low", "high"
# - Gemini 2.5 ä½¿ç”¨ thinking_budget: -1(é»˜è®¤), 0(å…³é—­), æˆ–æ­£æ•´æ•°
MODEL_CONFIGS = [
    # ===== Gemini 3 Flash æ¨¡å‹ï¼šæµ‹è¯•ä¸åŒ thinking_level =====
    # {
    #     "name": "gemini-3-flash (minimal)",
    #     "model": "gemini-3-flash-preview",
    #     "thinking_level": "minimal"  # æœ€å°åŒ–æ€è€ƒï¼ˆæ¨¡å‹å¯èƒ½ä¸ä¼šæ€è€ƒï¼‰
    # },
    # {
    #     "name": "gemini-3-flash (low)",
    #     "model": "gemini-3-flash-preview",
    #     "thinking_level": "low"  # ä½å¼ºåº¦æ€è€ƒ
    # },
    # {
    #     "name": "gemini-3-flash (high)",
    #     "model": "gemini-3-flash-preview",
    #     "thinking_level": "high"  # é«˜å¼ºåº¦æ€è€ƒï¼ˆé»˜è®¤ï¼‰
    # },

    # ===== Gemini 3 Pro æ¨¡å‹ï¼šåªæ”¯æŒ low å’Œ high =====
    # æ³¨æ„ï¼šPro æ— æ³•å®Œå…¨å…³é—­æ€è€ƒ
    {
        "name": "gemini-3-pro (low)",
        "model": "gemini-3-pro-preview",
        "thinking_level": "low"  # ä½å¼ºåº¦æ€è€ƒ
    },
    {
        "name": "gemini-3-pro (high)",
        "model": "gemini-3-pro-preview",
        "thinking_level": "high"  # é«˜å¼ºåº¦æ€è€ƒï¼ˆé»˜è®¤ï¼‰
    },

    # # ===== Gemini 2.5 Flash æ¨¡å‹ï¼šä½¿ç”¨ thinking_budget =====
    # {
    #     "name": "gemini-2.5-flash (budget=0)",
    #     "model": "gemini-2.5-flash",
    #     "thinking_budget": 0  # å…³é—­æ€è€ƒ
    # },
    # {
    #     "name": "gemini-2.5-flash (budget=2048)",
    #     "model": "gemini-2.5-flash",
    #     "thinking_budget": 2048  # ä¸­ç­‰é¢„ç®—
    # },
    # {
    #     "name": "gemini-2.5-flash (budget=-1)",
    #     "model": "gemini-2.5-flash",
    #     "thinking_budget": -1  # é»˜è®¤åŠ¨æ€æ€ç»´
    # },
]

# ä¸¤è½®å¯¹è¯çš„æç¤ºè¯ï¼ˆé™åˆ¶å›ç­”é•¿åº¦ä»¥åŠ å¿«æµ‹è¯•ï¼‰
PROMPTS = [
    "ä½ å¥½ï¼Œæˆ‘æœ€è¿‘å·¥ä½œå‹åŠ›å¾ˆå¤§ï¼Œæ„Ÿè§‰å¾ˆç„¦è™‘ã€‚è¯·ç”¨50ä¸ªå­—ä»¥å†…å›ç­”ã€‚",
    "è°¢è°¢ä½ çš„å»ºè®®ï¼Œæˆ‘è¯¥å¦‚ä½•å¼€å§‹æ”¹å–„è¿™ä¸ªçŠ¶å†µå‘¢ï¼Ÿè¯·ç”¨50ä¸ªå­—ä»¥å†…å›ç­”ã€‚"
]

# è¾“å‡º token é™åˆ¶
MAX_OUTPUT_TOKENS = 1000  # çº¦ç­‰äº 10 ä¸ªä¸­æ–‡å­—

# ä¸ä½¿ç”¨ç¡¬ç¼–ç çš„ç­‰å¾…æ—¶é—´ï¼Œå®Œå…¨ä¾èµ–åŒæ­¥æ‰§è¡Œ
# æ¯ä¸ªè¯·æ±‚ä¼šç­‰å¾…å®Œå…¨å®Œæˆåæ‰å¼€å§‹ä¸‹ä¸€ä¸ª


def test_model_with_timing(model: str, prompt: str, thinking_level: Optional[str] = None, thinking_budget: Optional[int] = None) -> Dict:
    """
    æµ‹è¯•å•ä¸ªæ¨¡å‹çš„å“åº”ï¼Œè®°å½•è¯¦ç»†æ—¶é—´æ•°æ®

    Args:
        model: æ¨¡å‹ID
        prompt: æç¤ºè¯
        thinking_level: thinking çº§åˆ«ï¼ˆ"minimal", "low", "high"ï¼‰- ç”¨äº Gemini 3
        thinking_budget: thinking é¢„ç®—ï¼ˆæ•´æ•°ï¼Œ-1=é»˜è®¤ï¼Œ0=å…³é—­ï¼‰- ç”¨äº Gemini 2.5

    Returns:
        {
            'first_token_time': é¦–å­—å»¶æ—¶ï¼ˆç§’ï¼‰,
            'total_time': æ€»å“åº”æ—¶é—´ï¼ˆç§’ï¼‰,
            'response_length': å“åº”å­—ç¬¦æ•°,
            'response_text': å“åº”å†…å®¹
        }
    """
    start_time = time.time()
    first_chunk_time = None  # ç¬¬ä¸€ä¸ª chunk åˆ°è¾¾æ—¶é—´
    first_token_time = None  # ç¬¬ä¸€ä¸ªæ–‡æœ¬ token åˆ°è¾¾æ—¶é—´
    response_text = ""
    chunk_count = 0

    try:
        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = {
            "model": model,
            "contents": prompt,
        }

        # æ„å»ºé…ç½®å‚æ•°
        config_kwargs = {
            "max_output_tokens": MAX_OUTPUT_TOKENS  # é™åˆ¶è¾“å‡ºé•¿åº¦
        }

        # å¦‚æœæä¾›äº† thinking é…ç½®ï¼Œæ·»åŠ åˆ° config ä¸­
        if thinking_level is not None or thinking_budget is not None:
            thinking_config_kwargs = {}

            if thinking_level is not None:
                thinking_config_kwargs["thinking_level"] = thinking_level
                print(f"    [è°ƒè¯•] Thinking é…ç½®: thinking_level={thinking_level}, max_tokens={MAX_OUTPUT_TOKENS}")
            elif thinking_budget is not None:
                thinking_config_kwargs["thinking_budget"] = thinking_budget
                print(f"    [è°ƒè¯•] Thinking é…ç½®: thinking_budget={thinking_budget}, max_tokens={MAX_OUTPUT_TOKENS}")

            config_kwargs["thinking_config"] = types.ThinkingConfig(**thinking_config_kwargs)
        else:
            print(f"    [è°ƒè¯•] æ—  Thinking é…ç½®, max_tokens={MAX_OUTPUT_TOKENS}")

        request_params["config"] = types.GenerateContentConfig(**config_kwargs)

        # ä½¿ç”¨æµå¼å“åº”æ¥è·å–é¦–å­—å»¶æ—¶
        response = client.models.generate_content_stream(**request_params)

        # æ¥æ”¶æµå¼å“åº”
        for chunk in response:
            chunk_count += 1
            current_time = time.time() - start_time

            # è®°å½•ç¬¬ä¸€ä¸ª chunk åˆ°è¾¾æ—¶é—´ï¼ˆæ— è®ºæ˜¯å¦æœ‰æ–‡æœ¬ï¼‰
            if first_chunk_time is None:
                first_chunk_time = current_time
                print(f"    [è°ƒè¯•] é¦–ä¸ª chunk åˆ°è¾¾: {first_chunk_time:.3f}ç§’")

            # æå–æ–‡æœ¬å†…å®¹
            if hasattr(chunk, 'candidates') and chunk.candidates:
                candidate = chunk.candidates[0]
                if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                    for part in candidate.content.parts:
                        # åªæå–æ–‡æœ¬éƒ¨åˆ†ï¼Œå¿½ç•¥ thought_signature ç­‰
                        if hasattr(part, 'text') and part.text:
                            # è®°å½•é¦–å­—å»¶æ—¶ï¼ˆç¬¬ä¸€ä¸ªåŒ…å«æ–‡æœ¬çš„ chunk åˆ°è¾¾æ—¶é—´ï¼‰
                            if first_token_time is None:
                                first_token_time = current_time
                                print(f"    [è°ƒè¯•] é¦–ä¸ªæ–‡æœ¬åˆ°è¾¾: {first_token_time:.3f}ç§’ (chunk #{chunk_count})")
                            response_text += part.text

        total_time = time.time() - start_time
        print(f"    [è°ƒè¯•] æ€» chunks: {chunk_count}, æ€»æ—¶é—´: {total_time:.3f}ç§’")

        return {
            'first_chunk_time': first_chunk_time or 0,
            'first_token_time': first_token_time or 0,
            'total_time': total_time,
            'response_length': len(response_text),
            'chunk_count': chunk_count,
            'response_text': response_text
        }

    except Exception as e:
        total_time = time.time() - start_time
        error_msg = str(e)
        print(f"    [é”™è¯¯] {error_msg}")
        return {
            'first_chunk_time': 0,
            'first_token_time': 0,
            'total_time': total_time,
            'response_length': 0,
            'chunk_count': 0,
            'response_text': f"é”™è¯¯: {error_msg}",
            'error': error_msg
        }


def test_network_latency():
    """æµ‹è¯•ç½‘ç»œå»¶è¿Ÿ"""
    print("\nğŸŒ æµ‹è¯•ç½‘ç»œå»¶è¿Ÿåˆ° Google API...")
    try:
        import urllib.request
        start = time.time()
        urllib.request.urlopen('https://generativelanguage.googleapis.com', timeout=10)
        latency = time.time() - start
        print(f"   ç½‘ç»œå»¶è¿Ÿ: {latency:.3f}ç§’")
        if latency > 1:
            print(f"   âš ï¸  ç½‘ç»œå»¶è¿Ÿè¾ƒé«˜ (>{latency:.1f}ç§’)")
        return latency
    except Exception as e:
        print(f"   âŒ ç½‘ç»œæµ‹è¯•å¤±è´¥: {e}")
        return None


def run_performance_test():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("\n" + "=" * 80)
    print("ğŸš€ Gemini æ¨¡å‹å»¶æ—¶æ€§èƒ½æµ‹è¯• - Thinking é…ç½®å¯¹æ¯”")
    print("=" * 80)
    print(f"ğŸ“… æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ æµ‹è¯•é…ç½®: {len(MODEL_CONFIGS)} ä¸ª")
    for config in MODEL_CONFIGS:
        print(f"    - {config['name']}")
    print(f"ğŸ’¬ å¯¹è¯è½®æ•°: {len(PROMPTS)}")
    print(f"ğŸ”„ æ‰§è¡Œæ¨¡å¼: å®Œå…¨ä¸²è¡Œ (æ¯ä¸ªè¯·æ±‚å®Œæˆåæ‰å¼€å§‹ä¸‹ä¸€ä¸ª)")
    print("=" * 80)

    # æµ‹è¯•ç½‘ç»œå»¶è¿Ÿ
    test_network_latency()
    print("=" * 80 + "\n")

    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = {}

    # ä¸²è¡Œæµ‹è¯•æ¯ä¸ªæ¨¡å‹é…ç½®ï¼ˆä¸€ä¸ªå®Œæˆåæ‰å¼€å§‹ä¸‹ä¸€ä¸ªï¼‰
    for config_idx, config in enumerate(MODEL_CONFIGS, 1):
        config_name = config['name']
        model_id = config['model']
        thinking_level = config.get('thinking_level')
        thinking_budget = config.get('thinking_budget')

        print(f"ğŸ“Š æµ‹è¯•é…ç½® {config_idx}/{len(MODEL_CONFIGS)}: {config_name}")
        print(f"   æ¨¡å‹: {model_id}")
        if thinking_level is not None:
            print(f"   Thinking Level: {thinking_level}")
        if thinking_budget is not None:
            print(f"   Thinking Budget: {thinking_budget}")
        print("-" * 80)

        model_results = {
            'model': model_id,
            'thinking_level': thinking_level,
            'thinking_budget': thinking_budget,
            'conversations': [],
            'total_length': 0,
            'total_time': 0
        }

        # è¿›è¡Œä¸¤è½®å¯¹è¯
        for round_num, prompt in enumerate(PROMPTS, 1):
            print(f"\nç¬¬ {round_num} è½®å¯¹è¯...")
            print(f"æç¤ºè¯: {prompt}")

            # æµ‹è¯•å“åº”
            result = test_model_with_timing(
                model_id,
                prompt,
                thinking_level=thinking_level,
                thinking_budget=thinking_budget
            )

            # æ‰“å°ç»“æœ
            print(f"â”œâ”€ é¦– chunk å»¶æ—¶: {result.get('first_chunk_time', 0):.3f}ç§’")
            print(f"â”œâ”€ é¦–æ–‡æœ¬å»¶æ—¶: {result['first_token_time']:.3f}ç§’")
            print(f"â”œâ”€ æ€»å“åº”æ—¶é—´: {result['total_time']:.3f}ç§’")
            print(f"â”œâ”€ å“åº”é•¿åº¦: {result['response_length']}å­—ç¬¦")
            print(f"â””â”€ Chunks æ•°é‡: {result.get('chunk_count', 0)}")

            model_results['conversations'].append(result)
            model_results['total_length'] += result['response_length']
            model_results['total_time'] += result['total_time']

            # ä¸æ·»åŠ äººå·¥ç­‰å¾…ï¼Œç›´æ¥è¿›å…¥ä¸‹ä¸€è½®ï¼ˆè‡ªç„¶ä¸²è¡Œæ‰§è¡Œï¼‰
            if round_num < len(PROMPTS):
                print()

        all_results[config_name] = model_results

        # ä¸æ·»åŠ äººå·¥ç­‰å¾…ï¼Œå½“å‰é…ç½®å®Œæˆåè‡ªåŠ¨å¼€å§‹ä¸‹ä¸€ä¸ª
        if config_idx < len(MODEL_CONFIGS):
            print(f"\nâœ… {config_name} æµ‹è¯•å®Œæˆï¼Œå¼€å§‹ä¸‹ä¸€ä¸ªé…ç½®...\n")

        print("=" * 80 + "\n")

    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print_comparison_table(all_results)

    # ä¿å­˜ç»“æœ
    save_results(all_results)

    return all_results


def print_comparison_table(results: Dict):
    """æ‰“å°æ€§èƒ½å¯¹æ¯”è¡¨æ ¼"""
    print("\n" + "=" * 120)
    print("ğŸ“Š æ€§èƒ½å¯¹æ¯”æ€»ç»“ - Thinking é…ç½®å½±å“")
    print("=" * 120 + "\n")

    # è¡¨å¤´
    header = f"{'é…ç½®åç§°':<35} {'é¦–å­—(R1)':<12} {'æ€»æ—¶(R1)':<12} {'é•¿åº¦(R1)':<10} {'é¦–å­—(R2)':<12} {'æ€»æ—¶(R2)':<12} {'é•¿åº¦(R2)':<10}"
    print(header)
    print("-" * 120)

    # éå†æ‰€æœ‰é…ç½®å¹¶æ‰“å°ç»“æœ
    for config_name, config_results in results.items():
        convs = config_results.get('conversations', [])

        # ç¬¬ä¸€è½®æ•°æ®
        conv1 = convs[0] if len(convs) > 0 else {}
        first_token_r1 = conv1.get('first_token_time', 0)
        total_time_r1 = conv1.get('total_time', 0)
        length_r1 = conv1.get('response_length', 0)

        # ç¬¬äºŒè½®æ•°æ®
        conv2 = convs[1] if len(convs) > 1 else {}
        first_token_r2 = conv2.get('first_token_time', 0)
        total_time_r2 = conv2.get('total_time', 0)
        length_r2 = conv2.get('response_length', 0)

        # æ‰“å°ä¸€è¡Œæ•°æ®
        row = f"{config_name:<35} {first_token_r1:>7.3f}ç§’   {total_time_r1:>7.3f}ç§’   {length_r1:>7}å­—   {first_token_r2:>7.3f}ç§’   {total_time_r2:>7.3f}ç§’   {length_r2:>7}å­—"
        print(row)

    print("-" * 120)

    # æ‰“å°åˆ†ç»„å¯¹æ¯”
    print("\nğŸ“ˆ åˆ†ç»„å¯¹æ¯”åˆ†æ:")
    print("-" * 120)

    # Gemini 3 Flash æ¨¡å‹å¯¹æ¯”
    print("\nğŸ”µ Gemini 3 Flash - Thinking Level å¯¹æ¯”:")
    flash3_configs = [
        "gemini-3-flash (minimal)",
        "gemini-3-flash (low)",
        "gemini-3-flash (high)"
    ]
    for config_name in flash3_configs:
        config_data = results.get(config_name, {})
        if config_data:
            print(f"   {config_name:<35} - æ€»æ—¶é—´: {config_data.get('total_time', 0):>6.3f}ç§’, æ€»é•¿åº¦: {config_data.get('total_length', 0):>5}å­—")

    # è®¡ç®—æ—¶é—´å·®å¼‚
    flash_min = results.get("gemini-3-flash (minimal)", {})
    flash_high = results.get("gemini-3-flash (high)", {})
    if flash_min and flash_high and flash_min.get('total_time', 0) > 0:
        time_diff = ((flash_high.get('total_time', 0) - flash_min.get('total_time', 0)) / flash_min.get('total_time', 1)) * 100
        print(f"   æ—¶é—´å·®å¼‚: {time_diff:+.1f}% (high vs minimal)")

    # Gemini 3 Pro æ¨¡å‹å¯¹æ¯”
    print("\nğŸŸ£ Gemini 3 Pro - Thinking Level å¯¹æ¯”:")
    pro3_configs = [
        "gemini-3-pro (low)",
        "gemini-3-pro (high)"
    ]
    for config_name in pro3_configs:
        config_data = results.get(config_name, {})
        if config_data:
            print(f"   {config_name:<35} - æ€»æ—¶é—´: {config_data.get('total_time', 0):>6.3f}ç§’, æ€»é•¿åº¦: {config_data.get('total_length', 0):>5}å­—")

    # Gemini 2.5 Flash æ¨¡å‹å¯¹æ¯”
    print("\nğŸŸ¢ Gemini 2.5 Flash - Thinking Budget å¯¹æ¯”:")
    flash25_configs = [
        "gemini-2.5-flash (budget=0)",
        "gemini-2.5-flash (budget=2048)",
        "gemini-2.5-flash (budget=-1)"
    ]
    for config_name in flash25_configs:
        config_data = results.get(config_name, {})
        if config_data:
            print(f"   {config_name:<35} - æ€»æ—¶é—´: {config_data.get('total_time', 0):>6.3f}ç§’, æ€»é•¿åº¦: {config_data.get('total_length', 0):>5}å­—")

    print("\n" + "=" * 120 + "\n")


def save_results(results: Dict):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"performance_test_{timestamp}.json"

    # ç®€åŒ–è¾“å‡ºï¼Œåªä¿ç•™æ—¶é—´æ•°æ®
    simplified_results = {}

    for model, data in results.items():
        simplified_results[model] = {
            'total_length': data['total_length'],
            'total_time': data['total_time'],
            'conversations': [
                {
                    'first_chunk_time': conv.get('first_chunk_time', 0),
                    'first_token_time': conv.get('first_token_time', 0),
                    'total_time': conv.get('total_time', 0),
                    'response_length': conv.get('response_length', 0),
                    'chunk_count': conv.get('chunk_count', 0)
                }
                for conv in data['conversations']
            ]
        }

    output = {
        'timestamp': timestamp,
        'test_type': 'thinking_config_comparison',
        'note': 'Testing Gemini 3 (thinking_level) and 2.5 (thinking_budget) models',
        'configurations': [
            {
                'name': config['name'],
                'model': config['model'],
                'thinking_level': config.get('thinking_level'),
                'thinking_budget': config.get('thinking_budget')
            }
            for config in MODEL_CONFIGS
        ],
        'conversation_rounds': len(PROMPTS),
        'results': simplified_results
    }

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"âœ… æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}\n")


if __name__ == "__main__":
    try:
        run_performance_test()
        print("âœ¨ æµ‹è¯•å®Œæˆï¼\n")
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•å·²è¢«ç”¨æˆ·ä¸­æ–­\n")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•å‡ºé”™: {str(e)}\n")
        import traceback
        traceback.print_exc()
