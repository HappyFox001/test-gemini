#!/usr/bin/env python3
"""
Gemini æ¨¡å‹å»¶æ—¶æ€§èƒ½æµ‹è¯•
æµ‹è¯•é¦–å­—å»¶æ—¶ã€æ€»å“åº”æ—¶é—´ã€å¯¹è¯é—´éš”
"""

import time
import json
import os
from datetime import datetime
from google import genai
from google.genai import types
from typing import Dict, Tuple, Optional

# åˆå§‹åŒ–å®¢æˆ·ç«¯
API_KEY = os.getenv('GEMINI_API_KEY')
if not API_KEY:
    raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ GEMINI_API_KEY")

client = genai.Client(api_key=API_KEY)

# æµ‹è¯•æ¨¡å‹é…ç½®åˆ—è¡¨ï¼ˆä¸²è¡Œæµ‹è¯•ï¼Œä¸€ä¸ªå®Œæˆåæ‰æµ‹ä¸‹ä¸€ä¸ªï¼‰
# æ¯ä¸ªé…ç½®åŒ…å«ï¼šname (æ˜¾ç¤ºåç§°), model (æ¨¡å‹ID), thinking_level (thinkingé…ç½®)
# æ³¨æ„ï¼šGemini 3 Pro æ— æ³•å®Œå…¨å…³é—­ thinkingï¼Œæœ€ä½åªèƒ½è®¾ç½®ä¸º LOW
MODEL_CONFIGS = [
    # Flash æ¨¡å‹ï¼šæµ‹è¯• LOW å’Œ HIGH thinkingï¼ˆFlash å¯ä»¥è®¾ç½®ä¸º 0 æ¥å…³é—­ï¼‰
    {
        "name": "gemini-3-flash (thinking OFF)",
        "model": "gemini-3-flash-preview",
        "thinking_budget": 0  # è®¾ç½®ä¸º 0 å…³é—­ thinking
    },
    {
        "name": "gemini-3-flash (thinking LOW)",
        "model": "gemini-3-flash-preview",
        "thinking_level": "low"  # ä½¿ç”¨å°å†™å­—ç¬¦ä¸²
    },
    {
        "name": "gemini-3-flash (thinking HIGH)",
        "model": "gemini-3-flash-preview",
        "thinking_level": "high"  # ä½¿ç”¨å°å†™å­—ç¬¦ä¸²
    },

    # Pro æ¨¡å‹ï¼šåªèƒ½æµ‹è¯• LOW å’Œ HIGHï¼ˆæ— æ³•å®Œå…¨å…³é—­ï¼‰
    {
        "name": "gemini-3-pro (thinking LOW)",
        "model": "gemini-3-pro-preview",
        "thinking_level": "low"  # ä½¿ç”¨å°å†™å­—ç¬¦ä¸²
    },
    {
        "name": "gemini-3-pro (thinking HIGH - default)",
        "model": "gemini-3-pro-preview",
        "thinking_level": "high"  # ä½¿ç”¨å°å†™å­—ç¬¦ä¸²ï¼Œè¿™æ˜¯é»˜è®¤å€¼
    },
]

# ä¸¤è½®å¯¹è¯çš„æç¤ºè¯
PROMPTS = [
    "ä½ å¥½ï¼Œæˆ‘æœ€è¿‘å·¥ä½œå‹åŠ›å¾ˆå¤§ï¼Œæ„Ÿè§‰å¾ˆç„¦è™‘ã€‚",
    "è°¢è°¢ä½ çš„å»ºè®®ï¼Œæˆ‘è¯¥å¦‚ä½•å¼€å§‹æ”¹å–„è¿™ä¸ªçŠ¶å†µå‘¢ï¼Ÿ"
]

# ä¸ä½¿ç”¨ç¡¬ç¼–ç çš„ç­‰å¾…æ—¶é—´ï¼Œå®Œå…¨ä¾èµ–åŒæ­¥æ‰§è¡Œ
# æ¯ä¸ªè¯·æ±‚ä¼šç­‰å¾…å®Œå…¨å®Œæˆåæ‰å¼€å§‹ä¸‹ä¸€ä¸ª


def test_model_with_timing(model: str, prompt: str, thinking_level = None, thinking_budget: Optional[int] = None) -> Dict:
    """
    æµ‹è¯•å•ä¸ªæ¨¡å‹çš„å“åº”ï¼Œè®°å½•è¯¦ç»†æ—¶é—´æ•°æ®

    Args:
        model: æ¨¡å‹ID
        prompt: æç¤ºè¯
        thinking_level: thinking çº§åˆ«ï¼ˆtypes.ThinkingLevel.LOW æˆ– types.ThinkingLevel.HIGHï¼‰
        thinking_budget: thinking é¢„ç®—ï¼ˆæ•´æ•°ï¼Œ0 è¡¨ç¤ºå…³é—­ï¼‰

    Returns:
        {
            'first_token_time': é¦–å­—å»¶æ—¶ï¼ˆç§’ï¼‰,
            'total_time': æ€»å“åº”æ—¶é—´ï¼ˆç§’ï¼‰,
            'response_length': å“åº”å­—ç¬¦æ•°,
            'response_text': å“åº”å†…å®¹
        }
    """
    start_time = time.time()
    first_chunk_time = None  # ç¬¬ä¸€ä¸ª chunk åˆ°è¾¾æ—¶é—´
    first_token_time = None  # ç¬¬ä¸€ä¸ªæ–‡æœ¬ token åˆ°è¾¾æ—¶é—´
    response_text = ""
    chunk_count = 0

    try:
        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = {
            "model": model,
            "contents": prompt,
        }

        # å¦‚æœæä¾›äº† thinking é…ç½®ï¼Œæ·»åŠ åˆ° config ä¸­
        if thinking_level is not None or thinking_budget is not None:
            thinking_config_kwargs = {}

            if thinking_budget is not None:
                thinking_config_kwargs["thinking_budget"] = thinking_budget
                print(f"    [è°ƒè¯•] Thinking é…ç½®: thinking_budget={thinking_budget}")
            elif thinking_level is not None:
                thinking_config_kwargs["thinking_level"] = thinking_level
                print(f"    [è°ƒè¯•] Thinking é…ç½®: thinking_level={thinking_level}")

            request_params["config"] = types.GenerateContentConfig(
                thinking_config=types.ThinkingConfig(**thinking_config_kwargs)
            )

        # ä½¿ç”¨æµå¼å“åº”æ¥è·å–é¦–å­—å»¶æ—¶
        response = client.models.generate_content_stream(**request_params)

        # æ¥æ”¶æµå¼å“åº”
        for chunk in response:
            chunk_count += 1
            current_time = time.time() - start_time

            # è®°å½•ç¬¬ä¸€ä¸ª chunk åˆ°è¾¾æ—¶é—´ï¼ˆæ— è®ºæ˜¯å¦æœ‰æ–‡æœ¬ï¼‰
            if first_chunk_time is None:
                first_chunk_time = current_time
                print(f"    [è°ƒè¯•] é¦–ä¸ª chunk åˆ°è¾¾: {first_chunk_time:.3f}ç§’")

            # æå–æ–‡æœ¬å†…å®¹
            if hasattr(chunk, 'candidates') and chunk.candidates:
                candidate = chunk.candidates[0]
                if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                    for part in candidate.content.parts:
                        # åªæå–æ–‡æœ¬éƒ¨åˆ†ï¼Œå¿½ç•¥ thought_signature ç­‰
                        if hasattr(part, 'text') and part.text:
                            # è®°å½•é¦–å­—å»¶æ—¶ï¼ˆç¬¬ä¸€ä¸ªåŒ…å«æ–‡æœ¬çš„ chunk åˆ°è¾¾æ—¶é—´ï¼‰
                            if first_token_time is None:
                                first_token_time = current_time
                                print(f"    [è°ƒè¯•] é¦–ä¸ªæ–‡æœ¬åˆ°è¾¾: {first_token_time:.3f}ç§’ (chunk #{chunk_count})")
                            response_text += part.text

        total_time = time.time() - start_time
        print(f"    [è°ƒè¯•] æ€» chunks: {chunk_count}, æ€»æ—¶é—´: {total_time:.3f}ç§’")

        return {
            'first_chunk_time': first_chunk_time or 0,
            'first_token_time': first_token_time or 0,
            'total_time': total_time,
            'response_length': len(response_text),
            'chunk_count': chunk_count,
            'response_text': response_text
        }

    except Exception as e:
        total_time = time.time() - start_time
        error_msg = str(e)
        print(f"    [é”™è¯¯] {error_msg}")
        return {
            'first_chunk_time': 0,
            'first_token_time': 0,
            'total_time': total_time,
            'response_length': 0,
            'chunk_count': 0,
            'response_text': f"é”™è¯¯: {error_msg}",
            'error': error_msg
        }


def test_network_latency():
    """æµ‹è¯•ç½‘ç»œå»¶è¿Ÿ"""
    print("\nğŸŒ æµ‹è¯•ç½‘ç»œå»¶è¿Ÿåˆ° Google API...")
    try:
        import urllib.request
        start = time.time()
        urllib.request.urlopen('https://generativelanguage.googleapis.com', timeout=10)
        latency = time.time() - start
        print(f"   ç½‘ç»œå»¶è¿Ÿ: {latency:.3f}ç§’")
        if latency > 1:
            print(f"   âš ï¸  ç½‘ç»œå»¶è¿Ÿè¾ƒé«˜ (>{latency:.1f}ç§’)")
        return latency
    except Exception as e:
        print(f"   âŒ ç½‘ç»œæµ‹è¯•å¤±è´¥: {e}")
        return None


def run_performance_test():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("\n" + "=" * 80)
    print("ğŸš€ Gemini æ¨¡å‹å»¶æ—¶æ€§èƒ½æµ‹è¯• - Thinking é…ç½®å¯¹æ¯”")
    print("=" * 80)
    print(f"ğŸ“… æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ æµ‹è¯•é…ç½®: {len(MODEL_CONFIGS)} ä¸ª")
    for config in MODEL_CONFIGS:
        print(f"    - {config['name']}")
    print(f"ğŸ’¬ å¯¹è¯è½®æ•°: {len(PROMPTS)}")
    print(f"ğŸ”„ æ‰§è¡Œæ¨¡å¼: å®Œå…¨ä¸²è¡Œ (æ¯ä¸ªè¯·æ±‚å®Œæˆåæ‰å¼€å§‹ä¸‹ä¸€ä¸ª)")
    print("=" * 80)

    # æµ‹è¯•ç½‘ç»œå»¶è¿Ÿ
    test_network_latency()
    print("=" * 80 + "\n")

    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = {}

    # ä¸²è¡Œæµ‹è¯•æ¯ä¸ªæ¨¡å‹é…ç½®ï¼ˆä¸€ä¸ªå®Œæˆåæ‰å¼€å§‹ä¸‹ä¸€ä¸ªï¼‰
    for config_idx, config in enumerate(MODEL_CONFIGS, 1):
        config_name = config['name']
        model_id = config['model']
        thinking_level = config.get('thinking_level')
        thinking_budget = config.get('thinking_budget')

        print(f"ğŸ“Š æµ‹è¯•é…ç½® {config_idx}/{len(MODEL_CONFIGS)}: {config_name}")
        print(f"   æ¨¡å‹: {model_id}")
        if thinking_level:
            print(f"   Thinking Level: {thinking_level}")
        if thinking_budget is not None:
            print(f"   Thinking Budget: {thinking_budget}")
        print("-" * 80)

        model_results = {
            'model': model_id,
            'thinking_level': thinking_level,
            'thinking_budget': thinking_budget,
            'conversations': [],
            'total_length': 0,
            'total_time': 0
        }

        # è¿›è¡Œä¸¤è½®å¯¹è¯
        for round_num, prompt in enumerate(PROMPTS, 1):
            print(f"\nç¬¬ {round_num} è½®å¯¹è¯...")
            print(f"æç¤ºè¯: {prompt}")

            # æµ‹è¯•å“åº”
            result = test_model_with_timing(
                model_id,
                prompt,
                thinking_level=thinking_level,
                thinking_budget=thinking_budget
            )

            # æ‰“å°ç»“æœ
            print(f"â”œâ”€ é¦– chunk å»¶æ—¶: {result.get('first_chunk_time', 0):.3f}ç§’")
            print(f"â”œâ”€ é¦–æ–‡æœ¬å»¶æ—¶: {result['first_token_time']:.3f}ç§’")
            print(f"â”œâ”€ æ€»å“åº”æ—¶é—´: {result['total_time']:.3f}ç§’")
            print(f"â”œâ”€ å“åº”é•¿åº¦: {result['response_length']}å­—ç¬¦")
            print(f"â””â”€ Chunks æ•°é‡: {result.get('chunk_count', 0)}")

            model_results['conversations'].append(result)
            model_results['total_length'] += result['response_length']
            model_results['total_time'] += result['total_time']

            # ä¸æ·»åŠ äººå·¥ç­‰å¾…ï¼Œç›´æ¥è¿›å…¥ä¸‹ä¸€è½®ï¼ˆè‡ªç„¶ä¸²è¡Œæ‰§è¡Œï¼‰
            if round_num < len(PROMPTS):
                print()

        all_results[config_name] = model_results

        # ä¸æ·»åŠ äººå·¥ç­‰å¾…ï¼Œå½“å‰é…ç½®å®Œæˆåè‡ªåŠ¨å¼€å§‹ä¸‹ä¸€ä¸ª
        if config_idx < len(MODEL_CONFIGS):
            print(f"\nâœ… {config_name} æµ‹è¯•å®Œæˆï¼Œå¼€å§‹ä¸‹ä¸€ä¸ªé…ç½®...\n")

        print("=" * 80 + "\n")

    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print_comparison_table(all_results)

    # ä¿å­˜ç»“æœ
    save_results(all_results)

    return all_results


def print_comparison_table(results: Dict):
    """æ‰“å°æ€§èƒ½å¯¹æ¯”è¡¨æ ¼"""
    print("\n" + "=" * 120)
    print("ğŸ“Š æ€§èƒ½å¯¹æ¯”æ€»ç»“ - Thinking é…ç½®å½±å“")
    print("=" * 120 + "\n")

    # è¡¨å¤´
    header = f"{'é…ç½®åç§°':<35} {'é¦–å­—(R1)':<12} {'æ€»æ—¶(R1)':<12} {'é•¿åº¦(R1)':<10} {'é¦–å­—(R2)':<12} {'æ€»æ—¶(R2)':<12} {'é•¿åº¦(R2)':<10}"
    print(header)
    print("-" * 120)

    # éå†æ‰€æœ‰é…ç½®å¹¶æ‰“å°ç»“æœ
    for config_name, config_results in results.items():
        convs = config_results.get('conversations', [])

        # ç¬¬ä¸€è½®æ•°æ®
        conv1 = convs[0] if len(convs) > 0 else {}
        first_token_r1 = conv1.get('first_token_time', 0)
        total_time_r1 = conv1.get('total_time', 0)
        length_r1 = conv1.get('response_length', 0)

        # ç¬¬äºŒè½®æ•°æ®
        conv2 = convs[1] if len(convs) > 1 else {}
        first_token_r2 = conv2.get('first_token_time', 0)
        total_time_r2 = conv2.get('total_time', 0)
        length_r2 = conv2.get('response_length', 0)

        # æ‰“å°ä¸€è¡Œæ•°æ®
        row = f"{config_name:<35} {first_token_r1:>7.3f}ç§’   {total_time_r1:>7.3f}ç§’   {length_r1:>7}å­—   {first_token_r2:>7.3f}ç§’   {total_time_r2:>7.3f}ç§’   {length_r2:>7}å­—"
        print(row)

    print("-" * 120)

    # æ‰“å°åˆ†ç»„å¯¹æ¯”
    print("\nğŸ“ˆ åˆ†ç»„å¯¹æ¯”åˆ†æ:")
    print("-" * 120)

    # Flash æ¨¡å‹å¯¹æ¯”
    print("\nğŸ”µ Flash æ¨¡å‹ - Thinking é…ç½®å¯¹æ¯”:")
    flash_configs = ["gemini-3-flash (thinking OFF)", "gemini-3-flash (thinking LOW)", "gemini-3-flash (thinking HIGH)"]
    flash_results_list = []
    for config_name in flash_configs:
        config_data = results.get(config_name, {})
        if config_data:
            print(f"   {config_name:<35} - æ€»æ—¶é—´: {config_data.get('total_time', 0):>6.3f}ç§’, æ€»é•¿åº¦: {config_data.get('total_length', 0):>5}å­—")
            flash_results_list.append((config_name, config_data))

    # è®¡ç®— Flash çš„æ—¶é—´å·®å¼‚
    if len(flash_results_list) >= 2:
        flash_off = results.get("gemini-3-flash (thinking OFF)", {})
        flash_high = results.get("gemini-3-flash (thinking HIGH)", {})
        if flash_off and flash_high and flash_off.get('total_time', 0) > 0:
            time_diff = ((flash_high.get('total_time', 0) - flash_off.get('total_time', 0)) / flash_off.get('total_time', 1)) * 100
            print(f"   æ—¶é—´å·®å¼‚: {time_diff:+.1f}% (HIGH vs OFF)")

    # Pro æ¨¡å‹å¯¹æ¯”
    print("\nğŸŸ£ Pro æ¨¡å‹ - Thinking é…ç½®å¯¹æ¯” (æ³¨æ„ï¼šPro æ— æ³•å®Œå…¨å…³é—­ thinking):")
    pro_configs = ["gemini-3-pro (thinking LOW)", "gemini-3-pro (thinking HIGH - default)"]
    for config_name in pro_configs:
        config_data = results.get(config_name, {})
        if config_data:
            print(f"   {config_name:<35} - æ€»æ—¶é—´: {config_data.get('total_time', 0):>6.3f}ç§’, æ€»é•¿åº¦: {config_data.get('total_length', 0):>5}å­—")

    print("\n" + "=" * 120 + "\n")


def save_results(results: Dict):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"performance_test_{timestamp}.json"

    # ç®€åŒ–è¾“å‡ºï¼Œåªä¿ç•™æ—¶é—´æ•°æ®
    simplified_results = {}

    for model, data in results.items():
        simplified_results[model] = {
            'total_length': data['total_length'],
            'total_time': data['total_time'],
            'conversations': [
                {
                    'first_chunk_time': conv.get('first_chunk_time', 0),
                    'first_token_time': conv.get('first_token_time', 0),
                    'total_time': conv.get('total_time', 0),
                    'response_length': conv.get('response_length', 0),
                    'chunk_count': conv.get('chunk_count', 0)
                }
                for conv in data['conversations']
            ]
        }

    output = {
        'timestamp': timestamp,
        'test_type': 'thinking_config_comparison',
        'configurations': [
            {
                'name': config['name'],
                'model': config['model'],
                'thinking_level': config.get('thinking_level'),
                'thinking_budget': config.get('thinking_budget')
            }
            for config in MODEL_CONFIGS
        ],
        'conversation_rounds': len(PROMPTS),
        'results': simplified_results
    }

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"âœ… æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}\n")


if __name__ == "__main__":
    try:
        run_performance_test()
        print("âœ¨ æµ‹è¯•å®Œæˆï¼\n")
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•å·²è¢«ç”¨æˆ·ä¸­æ–­\n")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•å‡ºé”™: {str(e)}\n")
        import traceback
        traceback.print_exc()
