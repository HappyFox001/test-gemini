#!/usr/bin/env python3
"""
Gemini æ¨¡å‹å»¶æ—¶æ€§èƒ½æµ‹è¯•
æµ‹è¯•é¦–å­—å»¶æ—¶ã€æ€»å“åº”æ—¶é—´ã€å¯¹è¯é—´éš”
"""

import time
import json
import os
from datetime import datetime
from google import genai
from typing import Dict, Tuple

# åˆå§‹åŒ–å®¢æˆ·ç«¯
API_KEY = os.getenv('GEMINI_API_KEY')
if not API_KEY:
    raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ GEMINI_API_KEY")

client = genai.Client(api_key=API_KEY)

# æµ‹è¯•æ¨¡å‹åˆ—è¡¨ï¼ˆä¸²è¡Œæµ‹è¯•ï¼Œä¸€ä¸ªå®Œæˆåæ‰æµ‹ä¸‹ä¸€ä¸ªï¼‰
MODELS = [
    "gemini-2.5-flash",
    "gemini-3-flash-preview",
    "gemini-3-pro-preview",
    "gemini-2.5-pro",
]

# ä¸¤è½®å¯¹è¯çš„æç¤ºè¯
PROMPTS = [
    "ä½ å¥½ï¼Œæˆ‘æœ€è¿‘å·¥ä½œå‹åŠ›å¾ˆå¤§ï¼Œæ„Ÿè§‰å¾ˆç„¦è™‘ã€‚",
    "è°¢è°¢ä½ çš„å»ºè®®ï¼Œæˆ‘è¯¥å¦‚ä½•å¼€å§‹æ”¹å–„è¿™ä¸ªçŠ¶å†µå‘¢ï¼Ÿ"
]

# ä¸ä½¿ç”¨ç¡¬ç¼–ç çš„ç­‰å¾…æ—¶é—´ï¼Œå®Œå…¨ä¾èµ–åŒæ­¥æ‰§è¡Œ
# æ¯ä¸ªè¯·æ±‚ä¼šç­‰å¾…å®Œå…¨å®Œæˆåæ‰å¼€å§‹ä¸‹ä¸€ä¸ª


def test_model_with_timing(model: str, prompt: str) -> Dict:
    """
    æµ‹è¯•å•ä¸ªæ¨¡å‹çš„å“åº”ï¼Œè®°å½•è¯¦ç»†æ—¶é—´æ•°æ®

    Returns:
        {
            'first_token_time': é¦–å­—å»¶æ—¶ï¼ˆç§’ï¼‰,
            'total_time': æ€»å“åº”æ—¶é—´ï¼ˆç§’ï¼‰,
            'response_length': å“åº”å­—ç¬¦æ•°,
            'response_text': å“åº”å†…å®¹
        }
    """
    start_time = time.time()
    first_chunk_time = None  # ç¬¬ä¸€ä¸ª chunk åˆ°è¾¾æ—¶é—´
    first_token_time = None  # ç¬¬ä¸€ä¸ªæ–‡æœ¬ token åˆ°è¾¾æ—¶é—´
    response_text = ""
    chunk_count = 0

    try:
        # ä½¿ç”¨æµå¼å“åº”æ¥è·å–é¦–å­—å»¶æ—¶
        response = client.models.generate_content_stream(
            model=model,
            contents=prompt,
        )

        # æ¥æ”¶æµå¼å“åº”
        for chunk in response:
            chunk_count += 1
            current_time = time.time() - start_time

            # è®°å½•ç¬¬ä¸€ä¸ª chunk åˆ°è¾¾æ—¶é—´ï¼ˆæ— è®ºæ˜¯å¦æœ‰æ–‡æœ¬ï¼‰
            if first_chunk_time is None:
                first_chunk_time = current_time
                print(f"    [è°ƒè¯•] é¦–ä¸ª chunk åˆ°è¾¾: {first_chunk_time:.3f}ç§’")

            # æå–æ–‡æœ¬å†…å®¹
            if hasattr(chunk, 'candidates') and chunk.candidates:
                candidate = chunk.candidates[0]
                if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                    for part in candidate.content.parts:
                        # åªæå–æ–‡æœ¬éƒ¨åˆ†ï¼Œå¿½ç•¥ thought_signature ç­‰
                        if hasattr(part, 'text') and part.text:
                            # è®°å½•é¦–å­—å»¶æ—¶ï¼ˆç¬¬ä¸€ä¸ªåŒ…å«æ–‡æœ¬çš„ chunk åˆ°è¾¾æ—¶é—´ï¼‰
                            if first_token_time is None:
                                first_token_time = current_time
                                print(f"    [è°ƒè¯•] é¦–ä¸ªæ–‡æœ¬åˆ°è¾¾: {first_token_time:.3f}ç§’ (chunk #{chunk_count})")
                            response_text += part.text

        total_time = time.time() - start_time
        print(f"    [è°ƒè¯•] æ€» chunks: {chunk_count}, æ€»æ—¶é—´: {total_time:.3f}ç§’")

        return {
            'first_chunk_time': first_chunk_time or 0,
            'first_token_time': first_token_time or 0,
            'total_time': total_time,
            'response_length': len(response_text),
            'chunk_count': chunk_count,
            'response_text': response_text
        }

    except Exception as e:
        total_time = time.time() - start_time
        error_msg = str(e)
        print(f"    [é”™è¯¯] {error_msg}")
        return {
            'first_chunk_time': 0,
            'first_token_time': 0,
            'total_time': total_time,
            'response_length': 0,
            'chunk_count': 0,
            'response_text': f"é”™è¯¯: {error_msg}",
            'error': error_msg
        }


def test_network_latency():
    """æµ‹è¯•ç½‘ç»œå»¶è¿Ÿ"""
    print("\nğŸŒ æµ‹è¯•ç½‘ç»œå»¶è¿Ÿåˆ° Google API...")
    try:
        import urllib.request
        start = time.time()
        urllib.request.urlopen('https://generativelanguage.googleapis.com', timeout=10)
        latency = time.time() - start
        print(f"   ç½‘ç»œå»¶è¿Ÿ: {latency:.3f}ç§’")
        if latency > 1:
            print(f"   âš ï¸  ç½‘ç»œå»¶è¿Ÿè¾ƒé«˜ (>{latency:.1f}ç§’)")
        return latency
    except Exception as e:
        print(f"   âŒ ç½‘ç»œæµ‹è¯•å¤±è´¥: {e}")
        return None


def run_performance_test():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("\n" + "=" * 80)
    print("ğŸš€ Gemini æ¨¡å‹å»¶æ—¶æ€§èƒ½æµ‹è¯• (å®Œå…¨ä¸²è¡Œ)")
    print("=" * 80)
    print(f"ğŸ“… æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ æµ‹è¯•æ¨¡å‹: {len(MODELS)} ä¸ª")
    print(f"    {', '.join(MODELS)}")
    print(f"ğŸ’¬ å¯¹è¯è½®æ•°: {len(PROMPTS)}")
    print(f"ğŸ”„ æ‰§è¡Œæ¨¡å¼: å®Œå…¨ä¸²è¡Œ (æ¯ä¸ªè¯·æ±‚å®Œæˆåæ‰å¼€å§‹ä¸‹ä¸€ä¸ª)")
    print("=" * 80)

    # æµ‹è¯•ç½‘ç»œå»¶è¿Ÿ
    test_network_latency()
    print("=" * 80 + "\n")

    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = {}

    # ä¸²è¡Œæµ‹è¯•æ¯ä¸ªæ¨¡å‹ï¼ˆä¸€ä¸ªå®Œæˆåæ‰å¼€å§‹ä¸‹ä¸€ä¸ªï¼‰
    for model_idx, model in enumerate(MODELS, 1):
        print(f"ğŸ“Š æµ‹è¯•æ¨¡å‹ {model_idx}/{len(MODELS)}: {model}")
        print("-" * 80)

        model_results = {
            'conversations': [],
            'total_length': 0,
            'total_time': 0
        }

        # è¿›è¡Œä¸¤è½®å¯¹è¯
        for round_num, prompt in enumerate(PROMPTS, 1):
            print(f"\nç¬¬ {round_num} è½®å¯¹è¯...")
            print(f"æç¤ºè¯: {prompt}")

            # æµ‹è¯•å“åº”
            result = test_model_with_timing(model, prompt)

            # æ‰“å°ç»“æœ
            print(f"â”œâ”€ é¦– chunk å»¶æ—¶: {result.get('first_chunk_time', 0):.3f}ç§’")
            print(f"â”œâ”€ é¦–æ–‡æœ¬å»¶æ—¶: {result['first_token_time']:.3f}ç§’")
            print(f"â”œâ”€ æ€»å“åº”æ—¶é—´: {result['total_time']:.3f}ç§’")
            print(f"â”œâ”€ å“åº”é•¿åº¦: {result['response_length']}å­—ç¬¦")
            print(f"â””â”€ Chunks æ•°é‡: {result.get('chunk_count', 0)}")

            model_results['conversations'].append(result)
            model_results['total_length'] += result['response_length']
            model_results['total_time'] += result['total_time']

            # ä¸æ·»åŠ äººå·¥ç­‰å¾…ï¼Œç›´æ¥è¿›å…¥ä¸‹ä¸€è½®ï¼ˆè‡ªç„¶ä¸²è¡Œæ‰§è¡Œï¼‰
            if round_num < len(PROMPTS):
                print()

        all_results[model] = model_results

        # ä¸æ·»åŠ äººå·¥ç­‰å¾…ï¼Œå½“å‰æ¨¡å‹å®Œæˆåè‡ªåŠ¨å¼€å§‹ä¸‹ä¸€ä¸ª
        if model_idx < len(MODELS):
            print(f"\nâœ… {model} æµ‹è¯•å®Œæˆï¼Œå¼€å§‹ä¸‹ä¸€ä¸ªæ¨¡å‹...\n")

        print("=" * 80 + "\n")

    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print_comparison_table(all_results)

    # ä¿å­˜ç»“æœ
    save_results(all_results)

    return all_results


def print_comparison_table(results: Dict):
    """æ‰“å°æ€§èƒ½å¯¹æ¯”è¡¨æ ¼"""
    print("\n" + "=" * 80)
    print("ğŸ“Š æ€§èƒ½å¯¹æ¯”æ€»ç»“")
    print("=" * 80 + "\n")

    # è¡¨å¤´
    print(f"{'æŒ‡æ ‡':<20} {'gemini-3-flash':<20} {'gemini-3-pro':<20}")
    print("-" * 80)

    # æå–ä¸¤ä¸ªæ¨¡å‹çš„ç»“æœ
    flash_key = "gemini-3-flash-preview"
    pro_key = "gemini-3-pro-preview"

    flash_results = results.get(flash_key, {})
    pro_results = results.get(pro_key, {})

    # ç¬¬ä¸€è½®å¯¹è¯æ•°æ®
    flash_conv1 = flash_results.get('conversations', [{}])[0]
    pro_conv1 = pro_results.get('conversations', [{}])[0]

    print(f"{'[ç¬¬1è½®] é¦–å­—å»¶æ—¶':<20} {flash_conv1.get('first_token_time', 0):.3f}ç§’{'':<13} {pro_conv1.get('first_token_time', 0):.3f}ç§’")
    print(f"{'[ç¬¬1è½®] æ€»æ—¶é—´':<20} {flash_conv1.get('total_time', 0):.3f}ç§’{'':<13} {pro_conv1.get('total_time', 0):.3f}ç§’")
    print(f"{'[ç¬¬1è½®] å“åº”é•¿åº¦':<20} {flash_conv1.get('response_length', 0)}å­—ç¬¦{'':<13} {pro_conv1.get('response_length', 0)}å­—ç¬¦")
    print()

    # ç¬¬äºŒè½®å¯¹è¯æ•°æ®
    flash_conv2 = flash_results.get('conversations', [{}])[1] if len(flash_results.get('conversations', [])) > 1 else {}
    pro_conv2 = pro_results.get('conversations', [{}])[1] if len(pro_results.get('conversations', [])) > 1 else {}

    print(f"{'[ç¬¬2è½®] é¦–å­—å»¶æ—¶':<20} {flash_conv2.get('first_token_time', 0):.3f}ç§’{'':<13} {pro_conv2.get('first_token_time', 0):.3f}ç§’")
    print(f"{'[ç¬¬2è½®] æ€»æ—¶é—´':<20} {flash_conv2.get('total_time', 0):.3f}ç§’{'':<13} {pro_conv2.get('total_time', 0):.3f}ç§’")
    print(f"{'[ç¬¬2è½®] å“åº”é•¿åº¦':<20} {flash_conv2.get('response_length', 0)}å­—ç¬¦{'':<13} {pro_conv2.get('response_length', 0)}å­—ç¬¦")

    print("-" * 80)

    # æ€»è®¡
    print(f"{'æ€»å“åº”é•¿åº¦':<20} {flash_results.get('total_length', 0)}å­—ç¬¦{'':<13} {pro_results.get('total_length', 0)}å­—ç¬¦")
    print(f"{'æ€»å“åº”æ—¶é—´':<20} {flash_results.get('total_time', 0):.3f}ç§’{'':<13} {pro_results.get('total_time', 0):.3f}ç§’")

    # è®¡ç®—å¹³å‡é€Ÿåº¦ï¼ˆå­—ç¬¦/ç§’ï¼‰
    flash_speed = flash_results.get('total_length', 0) / flash_results.get('total_time', 1) if flash_results.get('total_time', 0) > 0 else 0
    pro_speed = pro_results.get('total_length', 0) / pro_results.get('total_time', 1) if pro_results.get('total_time', 0) > 0 else 0

    print(f"{'å¹³å‡é€Ÿåº¦':<20} {flash_speed:.1f}å­—ç¬¦/ç§’{'':<8} {pro_speed:.1f}å­—ç¬¦/ç§’")

    print("\n" + "=" * 80 + "\n")


def save_results(results: Dict):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"performance_test_{timestamp}.json"

    # ç®€åŒ–è¾“å‡ºï¼Œåªä¿ç•™æ—¶é—´æ•°æ®
    simplified_results = {}

    for model, data in results.items():
        simplified_results[model] = {
            'total_length': data['total_length'],
            'total_time': data['total_time'],
            'conversations': [
                {
                    'first_chunk_time': conv.get('first_chunk_time', 0),
                    'first_token_time': conv.get('first_token_time', 0),
                    'total_time': conv.get('total_time', 0),
                    'response_length': conv.get('response_length', 0),
                    'chunk_count': conv.get('chunk_count', 0)
                }
                for conv in data['conversations']
            ]
        }

    output = {
        'timestamp': timestamp,
        'test_type': 'latency_performance',
        'models': MODELS,
        'conversation_rounds': len(PROMPTS),
        'results': simplified_results
    }

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"âœ… æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}\n")


if __name__ == "__main__":
    try:
        run_performance_test()
        print("âœ¨ æµ‹è¯•å®Œæˆï¼\n")
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•å·²è¢«ç”¨æˆ·ä¸­æ–­\n")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•å‡ºé”™: {str(e)}\n")
        import traceback
        traceback.print_exc()
