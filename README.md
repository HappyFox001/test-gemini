# Gemini 模型对比测试

测试 Gemini 2.5 Pro 和 3.0 Pro 在响应延时和情感交互质量上的表现。

## 安装依赖

```bash
pip install -r requirements.txt
```

## 配置 API Key

确保已设置 Google AI API Key 环境变量：

```bash
export GOOGLE_API_KEY="your-api-key-here"
```

或者在代码中配置（不推荐）。

## 运行测试

```bash
python test_gemini_models.py
```

## 测试内容

脚本会测试 6 个情感交互场景：

1. **表达焦虑寻求安慰** - 测试共情能力和安慰话术
2. **失败后需要鼓励** - 测试鼓励话术和情绪疏导
3. **分享喜悦** - 测试情感共鸣和互动自然度
4. **处理人际冲突** - 测试情感理解和建议实用性
5. **孤独感倾诉** - 测试倾听质量和共情深度
6. **自我怀疑** - 测试认知引导和鼓励方式

## 输出结果

- 控制台会实时显示每个测试的结果
- 测试完成后会显示总结报告，包括平均响应时间对比
- 详细结果会保存到 JSON 文件中（格式：`test_results_YYYYMMDD_HHMMSS.json`）

## 评估维度

- **响应延时**: 从发送请求到收到完整响应的时间
- **情感共鸣**: 模型是否能理解和回应用户的情感状态
- **回复质量**: 包括共情能力、建议实用性、语气温暖度等
- **自然度**: 交互是否自然流畅，像真实的人际对话

## 注意事项

- 每次测试之间会有 1 秒延迟，避免请求过快
- 如需修改测试用例，编辑脚本中的 `EMOTION_TEST_CASES` 列表
- 如需测试其他模型，修改 `MODELS` 列表
